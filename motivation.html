<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <title>Motivation</title>
    <link rel="stylesheet" href="styles_motivation.css">
</head>

<body>
    <a  href="index.html" class="home-button">
        <img style="float:right" src="icons8-home-24.png" alt="Home">
      </a>
    <div class="title">
    <div class="header">MOTIVATION</div>
  </div>

    <article class="all">
    <article class="inam">
    <h2>CURATING DATASET</h2>
    <p>
        The selection of the appropriate dataset is crucial for any data-driven project. This is why we paid a considerable amount of attention in the selection of dataset. 
        The dataset that we selected is relevant to our problem statement and it contains variables that impact our area of interest. We decided to find the highest total of 
        order returns per each book by analyzing the dataset. 
    </p>
    <p>The dataset that we selected contains the following characteristics. </p>
<h3>Quality:</h3> 
<p>It was achieved by cleaning the data from anomalies and raw data. We created a tool by using C# programming language to clean the data. </p>
<h3>Relevance to Problem Statement:</h3>
<p>The dataset should be directly related to the problem or question one is trying to solve. This dataset is about orders and its properties that we need to reach 
    the answer to our problem statement.</p>
<h3>Size and Sufficiency:</h3>
<p>The dataset needs to be large enough to provide statistical significance. On the other hand, if it is too small, it may not reflect the pattern in the dataset which can 
    lead to inaccurate conclusions. Therefore, the dataset we selected has a reasonable size containing around ten thousands records.</p>
<h3>Privacy and Compliance: </h3>
<p>The dataset is freely available for educational purpose, and we did not violate any regulations like GDPR or licensing terms.</p>
<h3>Tools used for cleaning dataset: </h3>
<p>The first task that we had after getting the dataset, is to clean it as it had so much raw and faulty data. 
    We decided to build a tool by using C# to process and clean the CSV file. This C# program is uploaded to the dataset folder for reference.
    This small program takes the csv file as an input stored on the disk and perform the clean operation and stores the cleaned file to the disk. 
</p>
<p>These are the tools we used for this purpose.</p>
<li>Microsoft Visual Studio</li>
<li>Visual Studio Code</li>


</p>

       
    </article>
    <div class="line"></div>
    <article class="waseem">
    <h2>DESIGNING ALGORITHM</h2>
        <p>Our process coming up with our algorithm began by brainstorming what kind of dataset we 
            should use. We started off with a few options; using data sourced from Web of Science 
            and maybe looking at article citations, using public statistics from a hospital, and finally, 
            and what we ultimately decided upon, using a dataset describing book purchases sourced from 
            a private company. From here we thought about what kind of problem we wanted to solve that 
            would fit our dataset. At first we wanted to create an algorithm which could calculate the 
            percentage of books returned, and maybe also the probability of a purchase being returned 
            depending on different variables. However, because of the amount of time and expertise that 
            these problems would require in order to solve, we decided in the end to design an algorithm 
            which can show the total number of returns per book title. </p>
        <p>We also discussed what coding language we should use for the algorithm. The dataset itself was 
            a CSV file and we wanted something that could be easily implemented into an HTML website. We 
            therefore decided to convert the CSV file into JSON and to use JavaScript for the algorithm 
            since it can be easily integrated into the HTML file. </p>
        <p>Waseem's text here... Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. Bunch of text. </p>
        <p>At the end of this process we came together in order to discuss the final version of the algorithm, 
            as well as the pseudocode describing it. This was a great opportunity for us to all get on the 
            same page as to what we wanted the algorithm to do, and exactly how the algorithm worked. After 
            making some minor adjustments to the pseudocode for clarity and simplicity, we felt that we were 
            happy with how the algorithm was able to solve our problem.</p>
    </article>
    <div class="line"></div>
    <article class="clara">
    <h2>CREATING WEBSITE</h2>
        <p>Using HTML and CSS when designing this website, we started off by creating a simple 
            homepage with a navigation bar, as well as four additional web pages which we link 
            to in our homepage. Our first concern has been to create a website which is functional 
            and will allow us to accomplish our assignment. This means using HTML to create headers 
            for easy navigation, making a table which fits our curated dataset and adding our 
            algorithm and additional information about our work. </p>
        <p>Our second concern has been to add CSS in order to improve some of the visuals and the layout
            of the website. We decided to make a separate CSS file for each HTML page so that it would 
            be easier to keep track of what styling changes are made to each page. These changes include 
            choosing font-family, designing the navigation bar, adding images, etc.</p>
        <p>Throughout the process of using HTML and CSS we have used W3 Schools as a source of inspiration. 
            By using the site’s many tutorials we have been able to learn more about how to apply HTML and 
            CSS, making adjustments in order to fit our specific needs. For example, W3 Schools have 
            tutorials on how to make so-called “cards” in HTML and CSS, which we have adapted for our 
            “About Us” page. We also used their tutorial on how to create cutout text in order to design 
            some of our pages’ headers. Using W3 Schools has been especially useful when the members of our 
            group come from different backgrounds and not necessarily on equal footing when it comes to 
            using these languages. It has given us the opportunity to learn more about the different 
            features one can add to a website, things that we otherwise would not have had the opportunity 
            to learn.</p>

</article>
</article>


</body>
</html>